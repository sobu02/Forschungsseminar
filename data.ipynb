{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowload and clean german Wikipedia dump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook ist das Hauptnotebook, welches den Wikipedia Dump herunterlädt, diesen aufsplittet in exzellente und nicht exzellente artikel und anschließend eine grundlegende Datenaufbereitung durchführt. Die Aufgaben 1 & 2 sind für eine bessere Übersicht in den folgenden seperaten Notebooks bearbeitet worden:\n",
    "\n",
    "Aufgabe 1: [Klassifizierung der Artikel](classification.ipynb)\n",
    "\n",
    "Aufgabe 2: [Keyword extraktion](keywords.ipynb)\n",
    "\n",
    "## Datenaufbereitung\n",
    "Die Datenaufbereitung für beide Aufgaben wurde in diesem Notebook durchgeführt. Dieses Notebook muss __nicht__ ausgeführt werden, um die Evaluation durchzuführen. Dafür ist ein Subset generiert worden und kann genutzt werden. Ausschließlich das Training des Bert Klassifizierungsmodell benötigt die vollständigen Daten. Das ausführen dieses Notebooks dauert viele Stunden! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html2text==2020.1.16 (from -r requirements.txt (line 1))\n",
      "  Using cached html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
      "Collecting mwxml==0.3.3 (from -r requirements.txt (line 2))\n",
      "  Using cached mwxml-0.3.3-py2.py3-none-any.whl (32 kB)\n",
      "Collecting wikitextparser==0.51.2 (from -r requirements.txt (line 3))\n",
      "  Using cached wikitextparser-0.51.2-py3-none-any.whl (65 kB)\n",
      "Collecting bz2file==0.98 (from -r requirements.txt (line 4))\n",
      "  Using cached bz2file-0.98.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting requests==2.30.0 (from -r requirements.txt (line 5))\n",
      "  Using cached requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "Collecting jsonschema>=2.5.1 (from mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading jsonschema-4.18.4-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mwcli>=0.0.2 (from mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Using cached mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Collecting mwtypes>=0.3.0 (from mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Using cached mwtypes-0.3.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting para>=0.0.1 (from mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Using cached para-0.0.8-py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: regex>=2022.9.11 in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from wikitextparser==0.51.2->-r requirements.txt (line 3)) (2023.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from wikitextparser==0.51.2->-r requirements.txt (line 3)) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (2022.6.15)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2)) (5.12.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading referencing-0.30.0-py3-none-any.whl (25 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading rpds_py-0.9.2-cp38-cp38-macosx_10_7_x86_64.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.9/311.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: docopt in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from mwcli>=0.0.2->mwxml==0.3.3->-r requirements.txt (line 2)) (0.6.2)\n",
      "Collecting jsonable>=0.3.0 (from mwtypes>=0.3.0->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Using cached jsonable-0.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/janmoormann/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2)) (3.11.0)\n",
      "Building wheels for collected packages: bz2file\n",
      "  Building wheel for bz2file (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6866 sha256=145e5c92b7f73aed18897ddfe1aed17d77bd3e39bf3c3c8227ea39f4a0c6ca3e\n",
      "  Stored in directory: /Users/janmoormann/Library/Caches/pip/wheels/19/88/ce/c9430af242507ffff602cf86c5ff6a1ae5205cba5aaf21f6cc\n",
      "Successfully built bz2file\n",
      "Installing collected packages: para, jsonable, bz2file, wikitextparser, rpds-py, requests, pkgutil-resolve-name, mwtypes, html2text, attrs, referencing, jsonschema-specifications, jsonschema, mwcli, mwxml\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.29.0\n",
      "    Uninstalling requests-2.29.0:\n",
      "      Successfully uninstalled requests-2.29.0\n",
      "Successfully installed attrs-23.1.0 bz2file-0.98 html2text-2020.1.16 jsonable-0.3.1 jsonschema-4.18.4 jsonschema-specifications-2023.7.1 mwcli-0.0.3 mwtypes-0.3.2 mwxml-0.3.3 para-0.0.8 pkgutil-resolve-name-1.3.10 referencing-0.30.0 requests-2.30.0 rpds-py-0.9.2 wikitextparser-0.51.2\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import requests\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# regex\n",
    "import re\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# read wikipedia dump\n",
    "import mwxml\n",
    "# data cleaning\n",
    "import html2text\n",
    "import wikitextparser as wtp\n",
    "# text metrics\n",
    "import textstat\n",
    "\n",
    "# multithreading\n",
    "from threading import Thread\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Variables / Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static var\n",
    "DUMP_URL = 'https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2'\n",
    "DUMP_FILE_ZIP = './dewiki-latest-pages-articles.xml.bz2'\n",
    "DUMP_FILE_ENTPACKT = './dewiki-latest-pages-articles.xml'\n",
    "\n",
    "EXZELLENT_FOLDER = './data/exzellent'\n",
    "NOT_EXZELLENT_FOLDER = './data/not_exzellent'\n",
    "SUBSET_FOLDER = './data/subset'\n",
    "\n",
    "CSV_FILE = './articles_meta.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download XML Dump herunterladen und Chunkweise abspeichern\n",
    "\n",
    "Herunterladen des Wikipedia Dumps mit allen deutschsprachigen Artikeln von Wikimedia. Es wurde sich gegen die API entschieden, da hier die Artikel alle einzeln heruntergeladen werden müssen und somit die Verarbeitungszeit für die 2 Mio. Artikel deutlich höher wäre. Der Dump entspricht der aktuellsten verfügbaren Version und enthält alle Wikipedia Artikel im XML-Format. Da die große Datei nicht auf einmal im Arbeitsspeicher geladen werden kann, wird diese in Chunks unterteilt und abgespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for downloading the Wikipedia dump chunk for chunk to reduce ram usage\n",
    "def download_file(url, file_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # write chunk in file\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "# download wikipedia dump\n",
    "download_file(DUMP_URL, DUMP_FILE_ZIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Dump entpacken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Wikipedia Dump nutzen zu können, muss dieser entpackt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the xml-dump and save it\n",
    "with open(DUMP_FILE_ENTPACKT, 'wb') as new_file, bz2.BZ2File(DUMP_FILE_ZIP, 'rb') as file:\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artikel aufbereiten und sortieren nach Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die bearbeitung der Aufgaben müssen die Artikel vorverarbeitet werden. Für eine einfachere Nutzung der Artikel werden diese aus der XML-Datei gelesen und in einzelnen Text Dateien gespeichert. Zudem wird der Text vorverarbeitet. Es werden diverse HTML und Markdown ähnliche Tags entfernt. Zudem werden bereits in der Aufbereitung einige features der Artikel berechnet bzw. gezählt und anschließend in einer CSV Datei abgelegt. Die einzelnen Artikel werden in eigenen Threads bearbeitet, um die Verarbeitungszeit zu verkürzen. Das zusammenführen der Features erfolgt wieder gesammelt im Main-Thread.\n",
    "\n",
    "Ein Beispiel-Artikel vor der Aufbereitung kann hier betrachtet werden: [explanation/160.xml](./explanation/160.xml) \n",
    "\n",
    "Der selbe Artikel nach der Aufbereitung finden Sie hier: [explanation/160.txt](./explanation/160.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanSaveArticleThread(Thread):\n",
    "    def __init__(self, *args):\n",
    "        Thread.__init__(self)\n",
    "        # get given args\n",
    "        self.page = args[0]\n",
    "        self.revision = args[1]\n",
    "\n",
    "        # initalize vars\n",
    "        self.number_images = 0\n",
    "        self.number_citations = 0\n",
    "        self.number_headers = 0\n",
    "        self.number_links = 0\n",
    "        self.number_categories = 0\n",
    "\n",
    "        self.saved = False\n",
    "        self.is_excellent = False\n",
    "\n",
    "        # set language of textstat\n",
    "        textstat.set_lang(\"de\")\n",
    "\n",
    "    # override the run function\n",
    "    def run(self):\n",
    "        # get text from revision\n",
    "        text = self.revision.text\n",
    "\n",
    "        # check if article is excellent\n",
    "        PATTERN_EXCELLENT = r\"\\{\\{Exzellent\\|\"\n",
    "        x = re.search(PATTERN_EXCELLENT, text)\n",
    "        if x is not None:\n",
    "            self.is_excellent = True\n",
    "        else:\n",
    "            self.is_excellent= False\n",
    "\n",
    "        # filter if article is only redirect and has no text \n",
    "        PATTERN_REDIRECT = r\"(#REDIRECT|#redirect|#WEITERLEITUNG)\"\n",
    "        \n",
    "        if re.search(PATTERN_REDIRECT, self.revision.text):\n",
    "            # with open(os.path.join('./data/trash', str(page.id) + '.txt'), \"x\") as f:\n",
    "            #     f.write(page.title + \"\\n\" + text)\n",
    "            self.saved = False\n",
    "            return\n",
    "\n",
    "\n",
    "        # feature extraction for classification task\n",
    "        # count images in article\n",
    "        PATTERN_IMAGES = r\"\\[\\[Datei:[^\\]]+\\.(?:jpg|png|svg)[^\\]]+\\]\\]\"\n",
    "        self.number_images = len(re.findall(PATTERN_IMAGES, self.revision.text))\n",
    "\n",
    "        # count citations in article\n",
    "        PATTERN_CITATIONS = r\"\\/ref\"\n",
    "        self.number_citations = len(re.findall(PATTERN_CITATIONS, self.revision.text))\n",
    "\n",
    "        # count headers\n",
    "        PATTERN_HEADER = r\"==+ (.*?) ==+\"\n",
    "        self.number_headers = len(re.findall(PATTERN_HEADER, self.revision.text))\n",
    "\n",
    "        # count link to other wikipedia articles\n",
    "        PATTERN_LINK = r\"\\[\\[(?!(?:.*\\bDatei:\\b.*|.*Kategorie:))([^]]+)\\]\\]\"\n",
    "        self.number_links = len(re.findall(PATTERN_LINK, self.revision.text))\n",
    "\n",
    "        # count categories of the article\n",
    "        PATTERN_CATEGORIE = r\"\\[\\[Kategorie:[^\\]]+\\]\\]\"\n",
    "        self.number_categories = len(re.findall(PATTERN_CATEGORIE, self.revision.text))\n",
    "\n",
    "\n",
    "        # text cleanup\n",
    "        # entnommen aus: https://github.com/daveshap/PlainTextWikipedia\n",
    "        try:\n",
    "            # Plain Text\n",
    "            text = wtp.parse(text).plain_text()  \n",
    "            # Remove HTML\n",
    "            text = html2text.html2text(text)\n",
    "        \n",
    "            # Replace newlines\n",
    "            text = text.replace('\\\\n', ' ')\n",
    "            # Replace excess whitespace\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "        except:\n",
    "            self.saved = False\n",
    "            return\n",
    "        # end entnommen aus\n",
    "\n",
    "        # calculate metrics / features for classification task\n",
    "        # count number of words\n",
    "        self.number_words = textstat.lexicon_count(text, removepunct=True)\n",
    "\n",
    "        # count number of scentens\n",
    "        self.number_scentens = textstat.sentence_count(text)\n",
    "\n",
    "        try:\n",
    "            # calculate Wiener Sachtextformel\n",
    "            self.wiener_sachtextformel = textstat.wiener_sachtextformel(text, variant=1)\n",
    "        except:\n",
    "            self.saved = False\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "        # save articles as txt file in correct folder\n",
    "        if self.is_excellent:\n",
    "            # filter excellent label from article (just to be sure is not in article anymore - usually the html2text function filtes these tags)\n",
    "            text = text.replace('\\{\\{Exzellent|', '\\{\\{')\n",
    "            # set target folder based on label\n",
    "            target_folder = EXZELLENT_FOLDER\n",
    "        else: \n",
    "            # set target folder based on label\n",
    "            target_folder = NOT_EXZELLENT_FOLDER\n",
    "        \n",
    "        # save in target folder and add Wikipedia title in first line of document\n",
    "        with open(os.path.join(target_folder, str(self.page.id) + '.txt'), \"x\") as f:\n",
    "            f.write(self.page.title + \"\\n\" + text)\n",
    "            f.close()\n",
    "            \n",
    "        # set article is saved var\n",
    "        self.saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meta_csv(thread: CleanSaveArticleThread) -> None:\n",
    "    # check if article is saved\n",
    "    if thread.saved:\n",
    "        # write meta data to csv file\n",
    "        with open(CSV_FILE, 'a') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\n",
    "                thread.page.id, \n",
    "                thread.is_excellent, \n",
    "                thread.number_images, \n",
    "                thread.number_citations, \n",
    "                thread.number_headers, \n",
    "                thread.number_links, \n",
    "                thread.number_categories,\n",
    "                thread.number_words,\n",
    "                thread.number_scentens,\n",
    "                thread.wiener_sachtextformel\n",
    "                ])\n",
    "            csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing existing folders and files\n",
      "### Wikipedia Dump ###\n",
      "Wikipedia dewiki\n",
      "### Read Articles ###\n",
      " -reading- Erfasste Artikel: 211689, davon vorverarbeitet: 211500   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -reading- Erfasste Artikel: 1922698, davon vorverarbeitet: 1922500   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -reading- Erfasste Artikel: 2091181, davon vorverarbeitet: 2091000   "
     ]
    }
   ],
   "source": [
    "# create exzellent folder if not exists otherwise remove existing folder\n",
    "print (\"removing existing folders and files\")\n",
    "if os.path.exists(EXZELLENT_FOLDER):\n",
    "    shutil.rmtree(EXZELLENT_FOLDER)\n",
    "os.makedirs(EXZELLENT_FOLDER)\n",
    "\n",
    "# create not exzellent folder if not exists otherwise remove existing folder\n",
    "if os.path.exists(NOT_EXZELLENT_FOLDER):\n",
    "    shutil.rmtree(NOT_EXZELLENT_FOLDER)\n",
    "os.makedirs(NOT_EXZELLENT_FOLDER)\n",
    "\n",
    "# create csv file for meta data\n",
    "header = [\n",
    "    'article_id',\n",
    "    'is_excellent',\n",
    "    'number_images',\n",
    "    'number_citations',\n",
    "    'number_headers',\n",
    "    'number_links',\n",
    "    'number_categories',\n",
    "    'number_words',\n",
    "    'number_scentens', \n",
    "    'wiener_sachtextformel'\n",
    "    ]\n",
    "\n",
    "# remove csv file if exists\n",
    "if os.path.exists(CSV_FILE):\n",
    "    os.remove(CSV_FILE)\n",
    "\n",
    "# write header to meta csv file\n",
    "with open(CSV_FILE, 'w+') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "\n",
    "# define wikipedia dump\n",
    "dump = mwxml.Dump.from_file(open(DUMP_FILE_ENTPACKT))\n",
    "\n",
    "count_articles = 0\n",
    "cleaned_saved = 0\n",
    "\n",
    "thread_list = []\n",
    "\n",
    "# print some information about the wikipedia dump\n",
    "print(\"### Wikipedia Dump ###\")\n",
    "print(dump.site_info.name, dump.site_info.dbname)\n",
    "\n",
    "print(\"### Read Articles ###\")\n",
    "# for schleifen entnommen aus: \n",
    "for idx_page, page in enumerate(dump):\n",
    "    for idx_revision, revision in enumerate(page):\n",
    "        if revision.text is not None:\n",
    "\n",
    "            count_articles += 1\n",
    "\n",
    "            # start basic cleaning of article in seperated Thread for better performance\n",
    "            new_thread = CleanSaveArticleThread(page, revision)\n",
    "            new_thread.start()\n",
    "            thread_list.append(new_thread)\n",
    "            \n",
    "            # update output\n",
    "            sys.stdout.write('\\r -reading- Erfasste Artikel: %i, davon vorverarbeitet: %i   ' % (count_articles, cleaned_saved))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "            # save all information in csv file\n",
    "            if(len(thread_list) >= 500):\n",
    "                sys.stdout.write('\\r -add csv- Erfasste Artikel: %i, davon vorverarbeitet: %i   ' % (count_articles, cleaned_saved))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                for thread in thread_list:\n",
    "                    # wait until thread ist done\n",
    "                    thread.join()\n",
    "                    \n",
    "                    # write data of thread to meta csv file\n",
    "                    write_meta_csv(thread)\n",
    "\n",
    "                    # increase number of saved files\n",
    "                    cleaned_saved += 1\n",
    "\n",
    "                # remove all threads \n",
    "                thread_list = []\n",
    "\n",
    "# save remaining article meta to csv\n",
    "for thread in thread_list:\n",
    "    # wait until thread ist done\n",
    "    thread.join()\n",
    "    \n",
    "    # write data of thread to meta csv file\n",
    "    write_meta_csv(thread)\n",
    "\n",
    "    # increase number of saved files\n",
    "    cleaned_saved += 1\n",
    "\n",
    "print('\\n Anzahl der erfassten Artikel: %i' % count_articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset generieren\n",
    "Zum generieren eines Subsets wird dieselbe train, val, test aufteilung genutzt, wie auch bei der classification Aufgabe. Es wird das Testdatenset als Subset abgespeichert. Dieses Subset ist zudem in dem GitHub Repository enthalten. Die Ergebnisse können dadurch bei der Evaluierug auch ohne herunterladen, entzippen und Datenaufbereitung des Wikipedia Dumps reproduziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove existing subset\n",
      "generate subset folders\n",
      "read meta data\n",
      "undersample original dataset\n",
      "split dataset\n",
      "save articles\n",
      "write new meta csv\n",
      "----\n",
      "Gesamtanzahl der Artikel: 2496675\n",
      "Anzahl der Samples im Subset (Testdaten): 807\n",
      "Klassenverteilung: Counter({True: 411, False: 396})\n"
     ]
    }
   ],
   "source": [
    "# generate subset equivalent to the test dataset \n",
    "print('remove existing subset')\n",
    "if os.path.exists(SUBSET_FOLDER):\n",
    "    shutil.rmtree(SUBSET_FOLDER)\n",
    "print('generate subset folders')\n",
    "os.makedirs(SUBSET_FOLDER)\n",
    "os.makedirs(SUBSET_FOLDER+'/exzellent')\n",
    "os.makedirs(SUBSET_FOLDER+'/not_exzellent')\n",
    "\n",
    "# read filenams from meta csv\n",
    "print('read meta data')\n",
    "original_meta_data = pd.read_csv(CSV_FILE, header=0, index_col=0)\n",
    "X = original_meta_data.drop(['is_excellent'], axis=1)\n",
    "Y = original_meta_data['is_excellent']\n",
    "\n",
    "# undersample data for same number of text per class \n",
    "print('undersample original dataset')\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X, Y = rus.fit_resample(X, Y)\n",
    "\n",
    "# gernerate train, val, test set\n",
    "print('split dataset')\n",
    "_, X_test_val, _, Y_test_val = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_test, _, Y_test, _ = train_test_split(X_test_val, Y_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "# concat x and y to df\n",
    "dataset = pd.concat([X_test,Y_test], axis=1)\n",
    "\n",
    "# save articles in subset folders\n",
    "print('save articles')\n",
    "for article_id, article in dataset.iterrows():\n",
    "    # get folder name based on label\n",
    "    ordner_original = EXZELLENT_FOLDER if article['is_excellent'] else NOT_EXZELLENT_FOLDER\n",
    "    ordner_subset = SUBSET_FOLDER + str('/exzellent' if article['is_excellent'] else '/not_exzellent')\n",
    "    # merge filename\n",
    "    filename = str(article_id) + '.txt'\n",
    "    # join filepath\n",
    "    filepath_original = os.path.join(ordner_original, filename)\n",
    "    filepath_subset = os.path.join(ordner_subset, filename)\n",
    "        \n",
    "    # copy original file to subset folder\n",
    "    shutil.copyfile(filepath_original, filepath_subset)\n",
    "\n",
    "# write meta csv for the subset\n",
    "print('write new meta csv')\n",
    "dataset.to_csv(os.path.join(SUBSET_FOLDER,'articles_meta.csv'), mode='w+')\n",
    "\n",
    "# print information about the subset\n",
    "print('----')\n",
    "print('Gesamtanzahl der Artikel: %s' % len(original_meta_data))\n",
    "print('Anzahl der Samples im Subset (Testdaten): %i' % len(Y_test))\n",
    "print('Klassenverteilung: %s' % Counter(Y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
