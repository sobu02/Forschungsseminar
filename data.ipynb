{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowload and clean german Wikipedia dump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook ist das Hauptnotebook, welches den Wikipedia Dump herunterlädt, diesen aufsplittet in exzellente und nicht exzellente artikel und anschließend eine grundlegende Datenaufbereitung durchführt. Die Aufgaben 1 & 2 sind für eine bessere Übersicht in den folgenden seperaten Notebooks bearbeitet worden:\n",
    "\n",
    "Aufgabe 1: Klassifizierung der Artikel \n",
    "\n",
    "Aufgabe 2: Keyword extraktion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# regex\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# package to read wikipedia dump\n",
    "import mwxml\n",
    "# packages for cleaning the data\n",
    "import html2text\n",
    "import wikitextparser as wtp\n",
    "\n",
    "# packages for multithreading\n",
    "from threading import Thread\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Variables / Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static var\n",
    "DUMP_URL = 'https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2'\n",
    "DUMP_FILE_ZIP = './dewiki-latest-pages-articles.xml.bz2'\n",
    "DUMP_FILE_ENTPACKT = './dewiki-latest-pages-articles.xml'\n",
    "\n",
    "EXZELLENT_FOLDER = './data/exzellent'\n",
    "NOT_EXZELLENT_FOLDER = './data/not_exzellent'\n",
    "SUBSET_FOLDER = './data/subset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_save_article(page: object, revision: object, is_excellent: bool) -> None:\n",
    "    text = revision.text\n",
    "\n",
    "    # filter if article is only redirect and has no text \n",
    "    PATTERN_REDIRECT = r\"(#REDIRECT|#redirect|#WEITERLEITUNG)\"\n",
    "    \n",
    "    if re.search(PATTERN_REDIRECT, revision.text):\n",
    "        # with open(os.path.join('./data/trash', str(page.id) + '.txt'), \"x\") as f:\n",
    "        #     f.write(page.title + \"\\n\" + text)\n",
    "        return\n",
    "\n",
    "    # entnommen aus: https://github.com/daveshap/PlainTextWikipedia\n",
    "    # Plain Text\n",
    "    text = wtp.parse(text).plain_text()  \n",
    "    # Remove HTML\n",
    "    text = html2text.html2text(text)\n",
    "    # Replace newlines\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    # Replace excess whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # end entnommen aus\n",
    "\n",
    "    \n",
    "    if is_excellent:\n",
    "        # filter excellent label from article (just to be sure is not in article anymore - usually the html2text function filtes these tags)\n",
    "        text = text.replace('\\{\\{Exzellent|', '\\{\\{')\n",
    "        # set target folder based on label\n",
    "        target_folder = EXZELLENT_FOLDER\n",
    "    else: \n",
    "        # set target folder based on label\n",
    "        target_folder = NOT_EXZELLENT_FOLDER\n",
    "    \n",
    "    # save in target folder and add Wikipedia title in first line of document\n",
    "    with open(os.path.join(target_folder, str(page.id) + '.txt'), \"x\") as f:\n",
    "        f.write(page.title + \"\\n\" + text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download XML Dump herunterladen und Chunkweise abspeichern\n",
    "\n",
    "Herunterladen des Wikipedia Dumps mit allen deutschsprachigen Artikeln von wikimedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Herunterladen der Datei\n",
    "def download_file(url, file_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "# Herunterladen des Wikipedia-Artikeldumps\n",
    "download_file(DUMP_URL, DUMP_FILE_ZIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Dump entpacken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DUMP_FILE_ENTPACKT, 'wb') as new_file, bz2.BZ2File(DUMP_FILE_ZIP, 'rb') as file:\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artikel aufbereiten und sortieren nach Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Wikipedia Dump ###\n",
      "Wikipedia dewiki\n",
      "### Exzelente Aritkel ###\n",
      " Verarbeitete Artikel: 6783 davon exzellent: 180"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9516:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/cb/gmv3s2j57kg6lbb4ystbns500000gn/T/ipykernel_86198/3083155661.py\", line 16, in clean_save_article\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/site-packages/html2text/__init__.py\", line 947, in html2text\n",
      "    return h.handle(html)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/site-packages/html2text/__init__.py\", line 142, in handle\n",
      "    self.feed(data)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/site-packages/html2text/__init__.py\", line 139, in feed\n",
      "    super().feed(data)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/html/parser.py\", line 110, in feed\n",
      "    self.goahead(0)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/html/parser.py\", line 178, in goahead\n",
      "    k = self.parse_html_declaration(i)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/html/parser.py\", line 263, in parse_html_declaration\n",
      "    return self.parse_marked_section(i)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/_markupbase.py\", line 149, in parse_marked_section\n",
      "    sectName, j = self._scan_name( i+3, i )\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/_markupbase.py\", line 391, in _scan_name\n",
      "    self.error(\"expected name token at %r\"\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/_markupbase.py\", line 33, in error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verarbeitete Artikel: 6784 davon exzellent: 180"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    raise NotImplementedError(\n",
      "NotImplementedError: subclasses of ParserBase must override error()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verarbeitete Artikel: 1134620 davon exzellent: 2140"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-1137353:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/cb/gmv3s2j57kg6lbb4ystbns500000gn/T/ipykernel_86198/3083155661.py\", line 16, in clean_save_article\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/site-packages/html2text/__init__.py\", line 947, in html2text\n",
      "    return h.handle(html)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/site-packages/html2text/__init__.py\", line 142, in handle\n",
      "    self.feed(data)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/site-packages/html2text/__init__.py\", line 139, in feed\n",
      "    super().feed(data)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/html/parser.py\", line 110, in feed\n",
      "    self.goahead(0)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/html/parser.py\", line 178, in goahead\n",
      "    k = self.parse_html_declaration(i)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/html/parser.py\", line 263, in parse_html_declaration\n",
      "    return self.parse_marked_section(i)\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/_markupbase.py\", line 159, in parse_marked_section\n",
      "    self.error('unknown status keyword %r in marked section' % rawdata[i+3:j])\n",
      "  File \"/Users/janmoormann/opt/anaconda3/envs/nlp_projekt/lib/python3.9/_markupbase.py\", line 33, in error\n",
      "    raise NotImplementedError(\n",
      "NotImplementedError: subclasses of ParserBase must override error()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verarbeitete Artikel: 2134844 davon exzellent: 2503"
     ]
    }
   ],
   "source": [
    "# pythonhosted.org\n",
    "PATTERN = r\"\\{\\{Exzellent\\|\"\n",
    "dump = mwxml.Dump.from_file(open(DUMP_FILE_ENTPACKT))\n",
    "\n",
    "excellent_count = 0\n",
    "not_excellent_count = 0\n",
    "\n",
    "print(\"### Wikipedia Dump ###\")\n",
    "print(dump.site_info.name, dump.site_info.dbname)\n",
    "\n",
    "print(\"### Exzelente Aritkel ###\")\n",
    "if not os.path.isdir(EXZELLENT_FOLDER):\n",
    "    os.makedirs(EXZELLENT_FOLDER)\n",
    "    \n",
    "# for schleifen entnommen aus: \n",
    "for idx_page, page in enumerate(dump):\n",
    "    for idx_revision, revision in enumerate(page):\n",
    "        if revision.text is not None:\n",
    "            x = re.search(PATTERN, revision.text)\n",
    "            if x is not None:\n",
    "                excellent_count += 1\n",
    "                is_excellent = True\n",
    "            else:\n",
    "                not_excellent_count += 1\n",
    "                is_excellent= False\n",
    "\n",
    "            # start basic cleaning of article in seperated Thread for better performance\n",
    "            Thread(target=clean_save_article, args=(page, revision, is_excellent)).start()\n",
    "\n",
    "            # update output\n",
    "            sys.stdout.write('\\r Verarbeitete Artikel: ' + str(excellent_count + not_excellent_count) + ' davon exzellent: ' + str(excellent_count))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "print('\\n Anzahl der exzellenten Artikel: ', str(excellent_count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamt Anzahl exzellenter Artikel:  2666629\n",
      "Gesamt Anzahl nicht exzellenter Artikel:  2689\n",
      "Gesamtanzahl Artikel:  2669318\n",
      "Anzahl exzellente Artikel Subset:  4\n",
      "Anzahl nicht exzellente Artikel Subset:  2997\n"
     ]
    }
   ],
   "source": [
    "# number overall articles\n",
    "SUBSET_SIZE = 3000\n",
    "\n",
    "articles_exzellent = os.listdir(EXZELLENT_FOLDER)\n",
    "articles_not_exzellent = os.listdir(NOT_EXZELLENT_FOLDER)\n",
    "\n",
    "number_exzellent:int = len(articles_exzellent)\n",
    "number_not_exzellent:int = len(articles_not_exzellent)\n",
    "\n",
    "ratio:float = float((number_exzellent / number_not_exzellent))\n",
    "\n",
    "print(\"Gesamt Anzahl exzellenter Artikel: \", number_not_exzellent)\n",
    "print(\"Gesamt Anzahl nicht exzellenter Artikel: \", number_exzellent)\n",
    "print(\"Gesamtanzahl Artikel: \", (number_exzellent + number_not_exzellent))\n",
    "\n",
    "for idx, article in enumerate(articles_exzellent):\n",
    "    shutil.copyfile(os.path.join(EXZELLENT_FOLDER,article), os.path.join(SUBSET_FOLDER,'exzellent',article))\n",
    "    if idx >= int(ratio * SUBSET_SIZE):\n",
    "        print(\"Anzahl exzellente Artikel Subset: \", idx + 1)\n",
    "        break\n",
    "\n",
    "for idx, article in enumerate(articles_not_exzellent):\n",
    "    shutil.copyfile(os.path.join(NOT_EXZELLENT_FOLDER,article), os.path.join(SUBSET_FOLDER,'not_exzellent',article))\n",
    "    if idx >= int((1 - ratio) * SUBSET_SIZE):\n",
    "        print(\"Anzahl nicht exzellente Artikel Subset: \", idx + 1)\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
