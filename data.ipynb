{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowload and clean german Wikipedia dump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook ist das Hauptnotebook, welches den Wikipedia Dump herunterlädt, diesen aufsplittet in exzellente und nicht exzellente artikel und anschließend eine grundlegende Datenaufbereitung durchführt. Die Aufgaben 1 & 2 sind für eine bessere Übersicht in den folgenden seperaten Notebooks bearbeitet worden:\n",
    "\n",
    "Aufgabe 1: [Klassifizierung der Artikel](classification.ipynb)\n",
    "\n",
    "Aufgabe 2: [Keyword extraktion](keywords.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: html2text==2020.1.16 in /opt/homebrew/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2020.1.16)\n",
      "Requirement already satisfied: mwxml==0.3.3 in /opt/homebrew/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.3.3)\n",
      "Requirement already satisfied: wikitextparser==0.51.2 in /opt/homebrew/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.51.2)\n",
      "Requirement already satisfied: bz2file==0.98 in /opt/homebrew/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.98)\n",
      "Requirement already satisfied: requests==2.30.0 in /opt/homebrew/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.30.0)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /opt/homebrew/lib/python3.9/site-packages (from mwxml==0.3.3->-r requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: mwcli>=0.0.2 in /opt/homebrew/lib/python3.9/site-packages (from mwxml==0.3.3->-r requirements.txt (line 2)) (0.0.3)\n",
      "Requirement already satisfied: mwtypes>=0.3.0 in /opt/homebrew/lib/python3.9/site-packages (from mwxml==0.3.3->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: para>=0.0.1 in /opt/homebrew/lib/python3.9/site-packages (from mwxml==0.3.3->-r requirements.txt (line 2)) (0.0.8)\n",
      "Requirement already satisfied: regex>=2022.9.11 in /opt/homebrew/lib/python3.9/site-packages (from wikitextparser==0.51.2->-r requirements.txt (line 3)) (2023.5.5)\n",
      "Requirement already satisfied: wcwidth in /Users/sophiabuehl/Library/Python/3.9/lib/python/site-packages (from wikitextparser==0.51.2->-r requirements.txt (line 3)) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.9/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/homebrew/lib/python3.9/site-packages (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2)) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/homebrew/lib/python3.9/site-packages (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: docopt in /opt/homebrew/lib/python3.9/site-packages (from mwcli>=0.0.2->mwxml==0.3.3->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: jsonable>=0.3.0 in /opt/homebrew/lib/python3.9/site-packages (from mwtypes>=0.3.0->mwxml==0.3.3->-r requirements.txt (line 2)) (0.3.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# regex\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import requests\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# package to read wikipedia dump\n",
    "import mwxml\n",
    "# packages for cleaning the data\n",
    "import html2text\n",
    "import wikitextparser as wtp\n",
    "\n",
    "import textstat\n",
    "\n",
    "# packages for multithreading\n",
    "from threading import Thread\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Variables / Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static var\n",
    "DUMP_URL = 'https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2'\n",
    "DUMP_FILE_ZIP = './dewiki-latest-pages-articles.xml.bz2'\n",
    "DUMP_FILE_ENTPACKT = './dewiki-latest-pages-articles.xml'\n",
    "\n",
    "EXZELLENT_FOLDER = './data/exzellent'\n",
    "NOT_EXZELLENT_FOLDER = './data/not_exzellent'\n",
    "SUBSET_FOLDER = './data/subset'\n",
    "\n",
    "CSV_FILE = './articles_meta.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download XML Dump herunterladen und Chunkweise abspeichern\n",
    "\n",
    "Herunterladen des Wikipedia Dumps mit allen deutschsprachigen Artikeln von wikimedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Herunterladen der Datei\n",
    "def download_file(url, file_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "# Herunterladen des Wikipedia-Artikeldumps\n",
    "download_file(DUMP_URL, DUMP_FILE_ZIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Dump entpacken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the xml-dump and save it\n",
    "with open(DUMP_FILE_ENTPACKT, 'wb') as new_file, bz2.BZ2File(DUMP_FILE_ZIP, 'rb') as file:\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artikel aufbereiten und sortieren nach Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Beispiel-Artikel vor der Aufbereitung kann hier betrachtet werden: [explanation/160.xml](./explanation/160.xml) \n",
    "\n",
    "Der selbe Artikel nach der Aufbereitung finden Sie hier: [explanation/160.txt](./explanation/160.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanSaveArticleThread(Thread):\n",
    "    def __init__(self, *args):\n",
    "        Thread.__init__(self)\n",
    "        self.page = args[0]\n",
    "        self.revision = args[1]\n",
    "        self.is_excellent = args[2]\n",
    "\n",
    "        self.number_images = 0\n",
    "        self.number_citations = 0\n",
    "        self.number_headers = 0\n",
    "        self.number_links = 0\n",
    "        self.number_categories = 0\n",
    "\n",
    "        self.saved = False\n",
    "\n",
    "        textstat.set_lang(\"de\")\n",
    "\n",
    "    # override the run function\n",
    "    def run(self):\n",
    "        \n",
    "        text = self.revision.text\n",
    "\n",
    "        # filter if article is only redirect and has no text \n",
    "        PATTERN_REDIRECT = r\"(#REDIRECT|#redirect|#WEITERLEITUNG)\"\n",
    "        \n",
    "        if re.search(PATTERN_REDIRECT, self.revision.text):\n",
    "            # with open(os.path.join('./data/trash', str(page.id) + '.txt'), \"x\") as f:\n",
    "            #     f.write(page.title + \"\\n\" + text)\n",
    "            self.saved = False\n",
    "            return\n",
    "\n",
    "\n",
    "        # feature extraction for classification task\n",
    "        # count images in article\n",
    "        PATTERN_IMAGES = r\"\\[\\[Datei:[^\\]]+\\.(?:jpg|png|svg)[^\\]]+\\]\\]\"\n",
    "        self.number_images = len(re.findall(PATTERN_IMAGES, self.revision.text))\n",
    "\n",
    "        # count citations in article\n",
    "        PATTERN_CITATIONS = r\"\\/ref\"\n",
    "        self.number_citations = len(re.findall(PATTERN_CITATIONS, self.revision.text))\n",
    "\n",
    "        # count headers\n",
    "        PATTERN_HEADER = r\"==+ (.*?) ==+\"\n",
    "        self.number_headers = len(re.findall(PATTERN_HEADER, self.revision.text))\n",
    "\n",
    "        # count link to other wikipedia articles\n",
    "        PATTERN_LINK = r\"\\[\\[(?!(?:.*\\bDatei:\\b.*|.*Kategorie:))([^]]+)\\]\\]\"\n",
    "        self.number_links = len(re.findall(PATTERN_LINK, self.revision.text))\n",
    "\n",
    "        # count categories of the article\n",
    "        PATTERN_CATEGORIE = r\"\\[\\[Kategorie:[^\\]]+\\]\\]\"\n",
    "        self.number_categories = len(re.findall(PATTERN_CATEGORIE, self.revision.text))\n",
    "\n",
    "\n",
    "        # text cleanup\n",
    "        # entnommen aus: https://github.com/daveshap/PlainTextWikipedia\n",
    "        try:\n",
    "            # Plain Text\n",
    "            text = wtp.parse(text).plain_text()  \n",
    "            # Remove HTML\n",
    "            text = html2text.html2text(text)\n",
    "        \n",
    "            # Replace newlines\n",
    "            text = text.replace('\\\\n', ' ')\n",
    "            # Replace excess whitespace\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "        except:\n",
    "            self.saved = False\n",
    "            return\n",
    "        # end entnommen aus\n",
    "\n",
    "        # calculate metrics / features for classification task\n",
    "        # count number of words\n",
    "        self.number_words = textstat.lexicon_count(text, removepunct=True)\n",
    "\n",
    "        # count number of scentens\n",
    "        self.number_scentens = textstat.sentence_count(text)\n",
    "\n",
    "        try:\n",
    "            # calculate Wiener Sachtextformel\n",
    "            self.wiener_sachtextformel = textstat.wiener_sachtextformel(text, variant=1)\n",
    "        except:\n",
    "            self.saved = False\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "        # save articles as txt file in correct folder\n",
    "        if self.is_excellent:\n",
    "            # filter excellent label from article (just to be sure is not in article anymore - usually the html2text function filtes these tags)\n",
    "            text = text.replace('\\{\\{Exzellent|', '\\{\\{')\n",
    "            # set target folder based on label\n",
    "            target_folder = EXZELLENT_FOLDER\n",
    "        else: \n",
    "            # set target folder based on label\n",
    "            target_folder = NOT_EXZELLENT_FOLDER\n",
    "        \n",
    "        # save in target folder and add Wikipedia title in first line of document\n",
    "        with open(os.path.join(target_folder, str(self.page.id) + '.txt'), \"x\") as f:\n",
    "            f.write(self.page.title + \"\\n\" + text)\n",
    "            f.close()\n",
    "            \n",
    "        self.saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meta_csv(thread: CleanSaveArticleThread):\n",
    "    # check if article is saved\n",
    "    if thread.saved:\n",
    "        # write meta data to csv file\n",
    "        with open(CSV_FILE, 'a') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\n",
    "                thread.page.id, \n",
    "                thread.is_excellent, \n",
    "                thread.number_images, \n",
    "                thread.number_citations, \n",
    "                thread.number_headers, \n",
    "                thread.number_links, \n",
    "                thread.number_categories,\n",
    "                thread.number_words,\n",
    "                thread.number_scentens,\n",
    "                thread.wiener_sachtextformel\n",
    "                ])\n",
    "            csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing existing folders and files\n",
      "### Wikipedia Dump ###\n",
      "Wikipedia dewiki\n",
      "### Exzelente Aritkel ###\n",
      " -reading- Erfasste Artikel: 41251 davon exzellent: 563 gespeichert: 41000"
     ]
    }
   ],
   "source": [
    "# create folder if not exists\n",
    "print (\"removing existing folders and files\")\n",
    "if os.path.exists(EXZELLENT_FOLDER):\n",
    "    shutil.rmtree(EXZELLENT_FOLDER)\n",
    "os.makedirs(EXZELLENT_FOLDER)\n",
    "\n",
    "if os.path.exists(NOT_EXZELLENT_FOLDER):\n",
    "    shutil.rmtree(NOT_EXZELLENT_FOLDER)\n",
    "os.makedirs(NOT_EXZELLENT_FOLDER)\n",
    "\n",
    "# create csv file for meta data\n",
    "header = [\n",
    "    'article_id',\n",
    "    'is_excellent',\n",
    "    'number_images',\n",
    "    'number_citations',\n",
    "    'number_headers',\n",
    "    'number_links',\n",
    "    'number_categories',\n",
    "    'number_words',\n",
    "    'number_scentens', \n",
    "    'wiener_sachtextformel'\n",
    "    ]\n",
    "\n",
    "if os.path.exists(CSV_FILE):\n",
    "    os.remove(CSV_FILE)\n",
    "\n",
    "with open(CSV_FILE, 'w+') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "pattern = r\"\\{\\{Exzellent\\|\"\n",
    "dump = mwxml.Dump.from_file(open(DUMP_FILE_ENTPACKT))\n",
    "\n",
    "excellent_count = 0\n",
    "not_excellent_count = 0\n",
    "cleaned_saved = 0\n",
    "\n",
    "thread_list = []\n",
    "\n",
    "print(\"### Wikipedia Dump ###\")\n",
    "print(dump.site_info.name, dump.site_info.dbname)\n",
    "\n",
    "print(\"### Exzelente Aritkel ###\")\n",
    "if not os.path.isdir(EXZELLENT_FOLDER):\n",
    "    os.makedirs(EXZELLENT_FOLDER)\n",
    "    \n",
    "# for schleifen entnommen aus: \n",
    "for idx_page, page in enumerate(dump):\n",
    "    for idx_revision, revision in enumerate(page):\n",
    "        if revision.text is not None:\n",
    "\n",
    "            x = re.search(pattern, revision.text)\n",
    "            if x is not None:\n",
    "                # finde the excellent articles\n",
    "                excellent_count += 1\n",
    "                is_excellent = True\n",
    "            else:\n",
    "                not_excellent_count += 1\n",
    "                is_excellent= False\n",
    "\n",
    "            # start basic cleaning of article in seperated Thread for better performance\n",
    "            new_thread = CleanSaveArticleThread(page, revision, is_excellent)\n",
    "            new_thread.start()\n",
    "            thread_list.append(new_thread)\n",
    "\n",
    "            new_thread.join()\n",
    "            \n",
    "            # update output\n",
    "            sys.stdout.write('\\r -reading- Erfasste Artikel: ' + str(excellent_count + not_excellent_count) + ' davon exzellent: ' + str(excellent_count) + ' gespeichert: ' + str(cleaned_saved))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "            # save all information in csv file\n",
    "            if(len(thread_list) >= 500):\n",
    "                sys.stdout.write('\\r Erfasste Artikel: ' + str(excellent_count + not_excellent_count) + ' davon exzellent: ' + str(excellent_count) + ' gespeichert: ' + str(cleaned_saved))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                for thread in thread_list:\n",
    "                    # wait until thread ist done\n",
    "                    thread.join()\n",
    "                    \n",
    "                    write_meta_csv(thread)\n",
    "\n",
    "                    cleaned_saved += 1\n",
    "\n",
    "                # remove all threads \n",
    "                thread_list = []\n",
    "\n",
    "# save remaining article meta to csv\n",
    "for thread in thread_list:\n",
    "    # wait until thread ist done\n",
    "    thread.join()\n",
    "    \n",
    "    write_meta_csv(thread)\n",
    "\n",
    "    cleaned_saved += 1\n",
    "\n",
    "print('\\n Anzahl der exzellenten Artikel: ', str(excellent_count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamt Anzahl exzellenter Artikel:  2666629\n",
      "Gesamt Anzahl nicht exzellenter Artikel:  2689\n",
      "Gesamtanzahl Artikel:  2669318\n",
      "Anzahl exzellente Artikel Subset:  4\n",
      "Anzahl nicht exzellente Artikel Subset:  2997\n"
     ]
    }
   ],
   "source": [
    "# number overall articles\n",
    "SUBSET_SIZE = 3000\n",
    "\n",
    "if not os.path.exists(SUBSET_FOLDER):\n",
    "    os.mkdir(SUBSET_FOLDER)\n",
    "\n",
    "    articles_exzellent = os.listdir(EXZELLENT_FOLDER)\n",
    "    articles_not_exzellent = os.listdir(NOT_EXZELLENT_FOLDER)\n",
    "\n",
    "    number_exzellent:int = len(articles_exzellent)\n",
    "    number_not_exzellent:int = len(articles_not_exzellent)\n",
    "\n",
    "    ratio:float = float((number_exzellent / number_not_exzellent))\n",
    "\n",
    "    print(\"Gesamt Anzahl exzellenter Artikel: \", number_not_exzellent)\n",
    "    print(\"Gesamt Anzahl nicht exzellenter Artikel: \", number_exzellent)\n",
    "    print(\"Gesamtanzahl Artikel: \", (number_exzellent + number_not_exzellent))\n",
    "\n",
    "    for idx, article in enumerate(articles_exzellent):\n",
    "        shutil.copyfile(os.path.join(EXZELLENT_FOLDER,article), os.path.join(SUBSET_FOLDER,'exzellent',article))\n",
    "        if idx >= int(ratio * SUBSET_SIZE):\n",
    "            print(\"Anzahl exzellente Artikel Subset: \", idx + 1)\n",
    "            break\n",
    "\n",
    "    for idx, article in enumerate(articles_not_exzellent):\n",
    "        shutil.copyfile(os.path.join(NOT_EXZELLENT_FOLDER,article), os.path.join(SUBSET_FOLDER,'not_exzellent',article))\n",
    "        if idx >= int((1 - ratio) * SUBSET_SIZE):\n",
    "            print(\"Anzahl nicht exzellente Artikel Subset: \", idx + 1)\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
