{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowload and clean german Wikipedia dump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook ist das Hauptnotebook, welches den Wikipedia Dump herunterlädt, diesen aufsplittet in exzellente und nicht exzellente artikel und anschließend eine grundlegende Datenaufbereitung durchführt. Die Aufgaben 1 & 2 sind für eine bessere Übersicht in den folgenden seperaten Notebooks bearbeitet worden:\n",
    "\n",
    "Aufgabe 1: [Klassifizierung der Artikel](classification.ipynb)\n",
    "\n",
    "Aufgabe 2: [Keyword extraktion](keywords.ipynb)\n",
    "\n",
    "## Datenaufbereitung\n",
    "Die Datenaufbereitung für beide Aufgaben wurde in diesem Notebook durchgeführt. Dieses Notebook muss __nicht__ ausgeführt werden, um die Evaluation durchzuführen. Dafür ist ein Subset generiert worden und kann genutzt werden. Ausschließlich das Training des Bert Klassifizierungsmodell benötigt die vollständigen Daten. Das ausführen dieses Notebooks dauert viele Stunden! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages\n",
    "Für Aplle Silicon Chips muss eine spezielle Version von pytorch verwendet werden. Daher werden die Installationen in den nachfolgenden Code-Blöcken unterschieden. Die Notebooks wurden mit einem Conda (Python 3.9) Kernel getestet und ausgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY for apple slicon\n",
    "! pip uninstall setuptools -y\n",
    "! conda install pytorch torchvision torchaudio -c pytorch-nightly -y\n",
    "! conda install numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY for other cpus\n",
    "! pip install numpy\n",
    "! pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site.py\", line 169, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n",
      "Collecting html2text==2020.1.16 (from -r requirements.txt (line 1))\n",
      "  Using cached html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
      "Collecting mwxml==0.3.3 (from -r requirements.txt (line 2))\n",
      "  Using cached mwxml-0.3.3-py2.py3-none-any.whl (32 kB)\n",
      "Collecting wikitextparser==0.51.2 (from -r requirements.txt (line 3))\n",
      "  Using cached wikitextparser-0.51.2-py3-none-any.whl (65 kB)\n",
      "Collecting bz2file==0.98 (from -r requirements.txt (line 4))\n",
      "  Using cached bz2file-0.98.tar.gz (11 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting requests==2.30.0 (from -r requirements.txt (line 5))\n",
      "  Using cached requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "Collecting pandas==1.4.2 (from -r requirements.txt (line 6))\n",
      "  Using cached pandas-1.4.2-cp39-cp39-macosx_11_0_arm64.whl (10.1 MB)\n",
      "Collecting sklearn==0.0.post5 (from -r requirements.txt (line 7))\n",
      "  Using cached sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting imbalanced-learn==0.11.0 (from -r requirements.txt (line 8))\n",
      "  Obtaining dependency information for imbalanced-learn==0.11.0 from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n",
      "  Using cached imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting textstat==0.7.3 (from -r requirements.txt (line 9))\n",
      "  Using cached textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "Collecting transformers==4.30.2 (from -r requirements.txt (line 10))\n",
      "  Obtaining dependency information for transformers==4.30.2 from https://files.pythonhosted.org/packages/5b/0b/e45d26ccd28568013523e04f325432ea88a442b4e3020b757cf4361f0120/transformers-4.30.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "Collecting torchmetrics==0.11.4 (from -r requirements.txt (line 11))\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchsummary==1.5.1 (from -r requirements.txt (line 12))\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Collecting tqdm==4.65.0 (from -r requirements.txt (line 13))\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting matplotlib==3.7.1 (from -r requirements.txt (line 14))\n",
      "  Downloading matplotlib-3.7.1-cp39-cp39-macosx_11_0_arm64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting seaborn==0.12.2 (from -r requirements.txt (line 15))\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonschema>=2.5.1 (from mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for jsonschema>=2.5.1 from https://files.pythonhosted.org/packages/a1/ba/28ce987450c6afa8336373761193ddaadc1ba2004fbf23a6407db036f558/jsonschema-4.18.4-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema-4.18.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting mwcli>=0.0.2 (from mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Collecting mwtypes>=0.3.0 (from mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading mwtypes-0.3.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting para>=0.0.1 (from mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading para-0.0.8-py3-none-any.whl (6.5 kB)\n",
      "Collecting regex>=2022.9.11 (from wikitextparser==0.51.2->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for regex>=2022.9.11 from https://files.pythonhosted.org/packages/7a/58/586acf6a0acc3584101865b8a8e6f434c645e3626f738dc28c59fca77c0d/regex-2023.6.3-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading regex-2023.6.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from wikitextparser==0.51.2->-r requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from requests==2.30.0->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from pandas==1.4.2->-r requirements.txt (line 6)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas==1.4.2->-r requirements.txt (line 6))\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from pandas==1.4.2->-r requirements.txt (line 6)) (1.25.0)\n",
      "Collecting scipy>=1.5.0 (from imbalanced-learn==0.11.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/d8/97/3a15209262cf523dab38de372ff814f8fb7815f98ccc07c7996e77910612/scipy-1.11.1-cp39-cp39-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scipy-1.11.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=1.0.2 (from imbalanced-learn==0.11.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for scikit-learn>=1.0.2 from https://files.pythonhosted.org/packages/63/87/6cd5450f0385966bf2a5b865a2043cf68c2a41676193afdbccb40f8719dc/scikit_learn-1.3.0-cp39-cp39-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.1.1 (from imbalanced-learn==0.11.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/28/08/9dcdaa5aac4634e4c23af26d92121f7ce445c630efa0d3037881ae2407fb/joblib-1.3.1-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from imbalanced-learn==0.11.0->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting pyphen (from textstat==0.7.3->-r requirements.txt (line 9))\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.2->-r requirements.txt (line 10))\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2->-r requirements.txt (line 10))\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.30.2->-r requirements.txt (line 10))\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-macosx_12_0_arm64.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from torchmetrics==0.11.4->-r requirements.txt (line 11)) (2.1.0.dev20230731)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.7.1->-r requirements.txt (line 14))\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/1b/26/192990fa4d10747d59c34d9eac2da0e045ae80aff9ae8a3e8d198146f11f/contourpy-1.1.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading contourpy-1.1.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.7.1->-r requirements.txt (line 14))\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.7.1->-r requirements.txt (line 14))\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/de/03/8a5a290c541ffcb6c7e56a1f7acf94d07389c4782d79e290449c9f9c03a5/fonttools-4.41.1-cp39-cp39-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading fonttools-4.41.1-cp39-cp39-macosx_10_9_universal2.whl.metadata (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.0/150.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib==3.7.1->-r requirements.txt (line 14))\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-macosx_11_0_arm64.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from matplotlib==3.7.1->-r requirements.txt (line 14)) (9.4.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.7.1->-r requirements.txt (line 14))\n",
      "  Obtaining dependency information for pyparsing>=2.3.1 from https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl.metadata\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib==3.7.1->-r requirements.txt (line 14))\n",
      "  Obtaining dependency information for importlib-resources>=3.2.0 from https://files.pythonhosted.org/packages/29/d1/bed03eca30aa05aaf6e0873de091f9385c48705c4a607c2dfe3edbe543e8/importlib_resources-6.0.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 10))\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 10)) (4.7.1)\n",
      "Collecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib==3.7.1->-r requirements.txt (line 14))\n",
      "  Obtaining dependency information for zipp>=3.1.0 from https://files.pythonhosted.org/packages/8c/08/d3006317aefe25ea79d3b76c9650afabaf6d63d1c8443b236e7405447503/zipp-3.16.2-py3-none-any.whl.metadata\n",
      "  Downloading zipp-3.16.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/1c/24/83349ac2189cc2435e84da3f69ba3c97314d3c0622628e55171c6798ed80/jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/ea/c3/f75f0ce2cdacca3d68a70b1756635092a1add1002e34afb4895b9fb62598/referencing-0.30.0-py3-none-any.whl.metadata\n",
      "  Downloading referencing-0.30.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/b2/ba/5bf78562543e9ff6bcefb370089323a078bd07179b7dc6be6dbdab632430/rpds_py-0.9.2-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading rpds_py-0.9.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Collecting docopt (from mwcli>=0.0.2->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonable>=0.3.0 (from mwtypes>=0.3.0->mwxml==0.3.3->-r requirements.txt (line 2))\n",
      "  Downloading jsonable-0.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.4.2->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==0.11.4->-r requirements.txt (line 11)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==0.11.4->-r requirements.txt (line 11)) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==0.11.4->-r requirements.txt (line 11)) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from jinja2->torch>=1.8.1->torchmetrics==0.11.4->-r requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/anaconda3/envs/forschungsseminar/lib/python3.9/site-packages (from sympy->torch>=1.8.1->torchmetrics==0.11.4->-r requirements.txt (line 11)) (1.2.1)\n",
      "Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.0-cp39-cp39-macosx_11_0_arm64.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fonttools-4.41.1-cp39-cp39-macosx_10_9_universal2.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.0.0-py3-none-any.whl (31 kB)\n",
      "Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.18.4-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.6.3-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.0-cp39-cp39-macosx_12_0_arm64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.1-cp39-cp39-macosx_12_0_arm64.whl (29.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.6/29.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Downloading referencing-0.30.0-py3-none-any.whl (25 kB)\n",
      "Downloading rpds_py-0.9.2-cp39-cp39-macosx_11_0_arm64.whl (306 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached zipp-3.16.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: bz2file, sklearn, docopt\n",
      "  Building wheel for bz2file (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6867 sha256=0f51aa4c4c8b29aefa6e383750e211e1102e0932b68af8ecc103c619fb87d445\n",
      "  Stored in directory: /Users/jan/Library/Caches/pip/wheels/d9/b8/e1/45ef4a93bf3ae5a374c13809abc1f9e676f41d7603277e953b\n",
      "  Building wheel for sklearn (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2949 sha256=16548f5c270976c90102bb741394cb6f6957d3ee2b340907a5650aa5aea9fa1e\n",
      "  Stored in directory: /Users/jan/Library/Caches/pip/wheels/36/49/c9/2374f1dee1b599effabf63d948635e6608f62d0ccde027b7e2\n",
      "  Building wheel for docopt (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=d0edb3b5c50225515da9b9ba1300ba5733c7a1a66215ef0e32d456b933b7ca5f\n",
      "  Stored in directory: /Users/jan/Library/Caches/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built bz2file sklearn docopt\n",
      "Installing collected packages: torchsummary, tokenizers, sklearn, safetensors, pytz, para, jsonable, docopt, bz2file, zipp, tqdm, threadpoolctl, scipy, rpds-py, requests, regex, pyphen, pyparsing, mwtypes, kiwisolver, joblib, html2text, fsspec, fonttools, cycler, contourpy, attrs, wikitextparser, textstat, scikit-learn, referencing, pandas, importlib-resources, huggingface-hub, transformers, torchmetrics, matplotlib, jsonschema-specifications, imbalanced-learn, seaborn, jsonschema, mwcli, mwxml\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed attrs-23.1.0 bz2file-0.98 contourpy-1.1.0 cycler-0.11.0 docopt-0.6.2 fonttools-4.41.1 fsspec-2023.6.0 html2text-2020.1.16 huggingface-hub-0.16.4 imbalanced-learn-0.11.0 importlib-resources-6.0.0 joblib-1.3.1 jsonable-0.3.1 jsonschema-4.18.4 jsonschema-specifications-2023.7.1 kiwisolver-1.4.4 matplotlib-3.7.1 mwcli-0.0.3 mwtypes-0.3.2 mwxml-0.3.3 pandas-1.4.2 para-0.0.8 pyparsing-3.1.1 pyphen-0.14.0 pytz-2023.3 referencing-0.30.0 regex-2023.6.3 requests-2.30.0 rpds-py-0.9.2 safetensors-0.3.1 scikit-learn-1.3.0 scipy-1.11.1 seaborn-0.12.2 sklearn-0.0.post5 textstat-0.7.3 threadpoolctl-3.2.0 tokenizers-0.13.3 torchmetrics-0.11.4 torchsummary-1.5.1 tqdm-4.65.0 transformers-4.30.2 wikitextparser-0.51.2 zipp-3.16.2\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import requests\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# regex\n",
    "import re\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# read wikipedia dump\n",
    "import mwxml\n",
    "# data cleaning\n",
    "import html2text\n",
    "import wikitextparser as wtp\n",
    "# text metrics\n",
    "import textstat\n",
    "\n",
    "# multithreading\n",
    "from threading import Thread\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Variables / Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static var\n",
    "DUMP_URL = 'https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2'\n",
    "DUMP_FILE_ZIP = './dewiki-latest-pages-articles.xml.bz2'\n",
    "DUMP_FILE_ENTPACKT = './dewiki-latest-pages-articles.xml'\n",
    "\n",
    "EXZELLENT_FOLDER = './data/exzellent'\n",
    "NOT_EXZELLENT_FOLDER = './data/not_exzellent'\n",
    "SUBSET_FOLDER = './data/subset'\n",
    "\n",
    "CSV_FILE = './articles_meta.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download XML Dump herunterladen und Chunkweise abspeichern\n",
    "\n",
    "Herunterladen des Wikipedia Dumps mit allen deutschsprachigen Artikeln von Wikimedia. Es wurde sich gegen die API entschieden, da hier die Artikel alle einzeln heruntergeladen werden müssen und somit die Verarbeitungszeit für die 2 Mio. Artikel deutlich höher wäre. Der Dump entspricht der aktuellsten verfügbaren Version und enthält alle Wikipedia Artikel im XML-Format. Da die große Datei nicht auf einmal im Arbeitsspeicher geladen werden kann, wird diese in Chunks unterteilt und abgespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for downloading the Wikipedia dump chunk for chunk to reduce ram usage\n",
    "def download_file(url, file_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # write chunk in file\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "# download wikipedia dump\n",
    "download_file(DUMP_URL, DUMP_FILE_ZIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Dump entpacken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Wikipedia Dump nutzen zu können, muss dieser entpackt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the xml-dump and save it\n",
    "with open(DUMP_FILE_ENTPACKT, 'wb') as new_file, bz2.BZ2File(DUMP_FILE_ZIP, 'rb') as file:\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artikel aufbereiten und sortieren nach Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die bearbeitung der Aufgaben müssen die Artikel vorverarbeitet werden. Für eine einfachere Nutzung der Artikel werden diese aus der XML-Datei gelesen und in einzelnen Text Dateien gespeichert. Zudem wird der Text vorverarbeitet. Es werden diverse HTML und Markdown ähnliche Tags entfernt. Zudem werden bereits in der Aufbereitung einige features der Artikel berechnet bzw. gezählt und anschließend in einer CSV Datei abgelegt. Die einzelnen Artikel werden in eigenen Threads bearbeitet, um die Verarbeitungszeit zu verkürzen. Das zusammenführen der Features erfolgt wieder gesammelt im Main-Thread.\n",
    "\n",
    "Ein Beispiel-Artikel vor der Aufbereitung kann hier betrachtet werden: [explanation/160.xml](./explanation/160.xml) \n",
    "\n",
    "Der selbe Artikel nach der Aufbereitung finden Sie hier: [explanation/160.txt](./explanation/160.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanSaveArticleThread(Thread):\n",
    "    def __init__(self, *args):\n",
    "        Thread.__init__(self)\n",
    "        # get given args\n",
    "        self.page = args[0]\n",
    "        self.revision = args[1]\n",
    "\n",
    "        # initalize vars\n",
    "        self.number_images = 0\n",
    "        self.number_citations = 0\n",
    "        self.number_headers = 0\n",
    "        self.number_links = 0\n",
    "        self.number_categories = 0\n",
    "\n",
    "        self.saved = False\n",
    "        self.is_excellent = False\n",
    "\n",
    "        # set language of textstat\n",
    "        textstat.set_lang(\"de\")\n",
    "\n",
    "    # override the run function\n",
    "    def run(self):\n",
    "        # get text from revision\n",
    "        text = self.revision.text\n",
    "\n",
    "        # check if article is excellent\n",
    "        PATTERN_EXCELLENT = r\"\\{\\{Exzellent\\|\"\n",
    "        x = re.search(PATTERN_EXCELLENT, text)\n",
    "        if x is not None:\n",
    "            self.is_excellent = True\n",
    "        else:\n",
    "            self.is_excellent= False\n",
    "\n",
    "        # filter if article is only redirect and has no text \n",
    "        PATTERN_REDIRECT = r\"(#REDIRECT|#redirect|#WEITERLEITUNG)\"\n",
    "        \n",
    "        if re.search(PATTERN_REDIRECT, self.revision.text):\n",
    "            # with open(os.path.join('./data/trash', str(page.id) + '.txt'), \"x\") as f:\n",
    "            #     f.write(page.title + \"\\n\" + text)\n",
    "            self.saved = False\n",
    "            return\n",
    "\n",
    "\n",
    "        # feature extraction for classification task\n",
    "        # count images in article\n",
    "        PATTERN_IMAGES = r\"\\[\\[Datei:[^\\]]+\\.(?:jpg|png|svg)[^\\]]+\\]\\]\"\n",
    "        self.number_images = len(re.findall(PATTERN_IMAGES, self.revision.text))\n",
    "\n",
    "        # count citations in article\n",
    "        PATTERN_CITATIONS = r\"\\/ref\"\n",
    "        self.number_citations = len(re.findall(PATTERN_CITATIONS, self.revision.text))\n",
    "\n",
    "        # count headers\n",
    "        PATTERN_HEADER = r\"==+ (.*?) ==+\"\n",
    "        self.number_headers = len(re.findall(PATTERN_HEADER, self.revision.text))\n",
    "\n",
    "        # count link to other wikipedia articles\n",
    "        PATTERN_LINK = r\"\\[\\[(?!(?:.*\\bDatei:\\b.*|.*Kategorie:))([^]]+)\\]\\]\"\n",
    "        self.number_links = len(re.findall(PATTERN_LINK, self.revision.text))\n",
    "\n",
    "        # count categories of the article\n",
    "        PATTERN_CATEGORIE = r\"\\[\\[Kategorie:[^\\]]+\\]\\]\"\n",
    "        self.number_categories = len(re.findall(PATTERN_CATEGORIE, self.revision.text))\n",
    "\n",
    "\n",
    "        # text cleanup\n",
    "        # entnommen aus: https://github.com/daveshap/PlainTextWikipedia\n",
    "        try:\n",
    "            # Plain Text\n",
    "            text = wtp.parse(text).plain_text()  \n",
    "            # Remove HTML\n",
    "            text = html2text.html2text(text)\n",
    "        \n",
    "            # Replace newlines\n",
    "            text = text.replace('\\\\n', ' ')\n",
    "            # Replace excess whitespace\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "        except:\n",
    "            self.saved = False\n",
    "            return\n",
    "        # end entnommen aus\n",
    "\n",
    "        # calculate metrics / features for classification task\n",
    "        # count number of words\n",
    "        self.number_words = textstat.lexicon_count(text, removepunct=True)\n",
    "\n",
    "        # count number of scentens\n",
    "        self.number_scentens = textstat.sentence_count(text)\n",
    "\n",
    "        try:\n",
    "            # calculate Wiener Sachtextformel\n",
    "            self.wiener_sachtextformel = textstat.wiener_sachtextformel(text, variant=1)\n",
    "        except:\n",
    "            self.saved = False\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "        # save articles as txt file in correct folder\n",
    "        if self.is_excellent:\n",
    "            # filter excellent label from article (just to be sure is not in article anymore - usually the html2text function filtes these tags)\n",
    "            text = text.replace('\\{\\{Exzellent|', '\\{\\{')\n",
    "            # set target folder based on label\n",
    "            target_folder = EXZELLENT_FOLDER\n",
    "        else: \n",
    "            # set target folder based on label\n",
    "            target_folder = NOT_EXZELLENT_FOLDER\n",
    "        \n",
    "        # save in target folder and add Wikipedia title in first line of document\n",
    "        with open(os.path.join(target_folder, str(self.page.id) + '.txt'), \"x\") as f:\n",
    "            f.write(self.page.title + \"\\n\" + text)\n",
    "            f.close()\n",
    "            \n",
    "        # set article is saved var\n",
    "        self.saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meta_csv(thread: CleanSaveArticleThread) -> None:\n",
    "    # check if article is saved\n",
    "    if thread.saved:\n",
    "        # write meta data to csv file\n",
    "        with open(CSV_FILE, 'a') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\n",
    "                thread.page.id, \n",
    "                thread.is_excellent, \n",
    "                thread.number_images, \n",
    "                thread.number_citations, \n",
    "                thread.number_headers, \n",
    "                thread.number_links, \n",
    "                thread.number_categories,\n",
    "                thread.number_words,\n",
    "                thread.number_scentens,\n",
    "                thread.wiener_sachtextformel\n",
    "                ])\n",
    "            csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing existing folders and files\n",
      "### Wikipedia Dump ###\n",
      "Wikipedia dewiki\n",
      "### Read Articles ###\n",
      " -reading- Erfasste Artikel: 211689, davon vorverarbeitet: 211500   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -reading- Erfasste Artikel: 1922698, davon vorverarbeitet: 1922500   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -reading- Erfasste Artikel: 2091181, davon vorverarbeitet: 2091000   "
     ]
    }
   ],
   "source": [
    "# create exzellent folder if not exists otherwise remove existing folder\n",
    "print (\"removing existing folders and files\")\n",
    "if os.path.exists(EXZELLENT_FOLDER):\n",
    "    shutil.rmtree(EXZELLENT_FOLDER)\n",
    "os.makedirs(EXZELLENT_FOLDER)\n",
    "\n",
    "# create not exzellent folder if not exists otherwise remove existing folder\n",
    "if os.path.exists(NOT_EXZELLENT_FOLDER):\n",
    "    shutil.rmtree(NOT_EXZELLENT_FOLDER)\n",
    "os.makedirs(NOT_EXZELLENT_FOLDER)\n",
    "\n",
    "# create csv file for meta data\n",
    "header = [\n",
    "    'article_id',\n",
    "    'is_excellent',\n",
    "    'number_images',\n",
    "    'number_citations',\n",
    "    'number_headers',\n",
    "    'number_links',\n",
    "    'number_categories',\n",
    "    'number_words',\n",
    "    'number_scentens', \n",
    "    'wiener_sachtextformel'\n",
    "    ]\n",
    "\n",
    "# remove csv file if exists\n",
    "if os.path.exists(CSV_FILE):\n",
    "    os.remove(CSV_FILE)\n",
    "\n",
    "# write header to meta csv file\n",
    "with open(CSV_FILE, 'w+') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "\n",
    "# define wikipedia dump\n",
    "dump = mwxml.Dump.from_file(open(DUMP_FILE_ENTPACKT))\n",
    "\n",
    "count_articles = 0\n",
    "cleaned_saved = 0\n",
    "\n",
    "thread_list = []\n",
    "\n",
    "# print some information about the wikipedia dump\n",
    "print(\"### Wikipedia Dump ###\")\n",
    "print(dump.site_info.name, dump.site_info.dbname)\n",
    "\n",
    "print(\"### Read Articles ###\")\n",
    "# for schleifen entnommen aus: \n",
    "for idx_page, page in enumerate(dump):\n",
    "    for idx_revision, revision in enumerate(page):\n",
    "        if revision.text is not None:\n",
    "\n",
    "            count_articles += 1\n",
    "\n",
    "            # start basic cleaning of article in seperated Thread for better performance\n",
    "            new_thread = CleanSaveArticleThread(page, revision)\n",
    "            new_thread.start()\n",
    "            thread_list.append(new_thread)\n",
    "            \n",
    "            # update output\n",
    "            sys.stdout.write('\\r -reading- Erfasste Artikel: %i, davon vorverarbeitet: %i   ' % (count_articles, cleaned_saved))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "            # save all information in csv file\n",
    "            if(len(thread_list) >= 500):\n",
    "                sys.stdout.write('\\r -add csv- Erfasste Artikel: %i, davon vorverarbeitet: %i   ' % (count_articles, cleaned_saved))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                for thread in thread_list:\n",
    "                    # wait until thread ist done\n",
    "                    thread.join()\n",
    "                    \n",
    "                    # write data of thread to meta csv file\n",
    "                    write_meta_csv(thread)\n",
    "\n",
    "                    # increase number of saved files\n",
    "                    cleaned_saved += 1\n",
    "\n",
    "                # remove all threads \n",
    "                thread_list = []\n",
    "\n",
    "# save remaining article meta to csv\n",
    "for thread in thread_list:\n",
    "    # wait until thread ist done\n",
    "    thread.join()\n",
    "    \n",
    "    # write data of thread to meta csv file\n",
    "    write_meta_csv(thread)\n",
    "\n",
    "    # increase number of saved files\n",
    "    cleaned_saved += 1\n",
    "\n",
    "print('\\n Anzahl der erfassten Artikel: %i' % count_articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset generieren\n",
    "Zum generieren eines Subsets wird dieselbe train, val, test aufteilung genutzt, wie auch bei der classification Aufgabe. Es wird das Testdatenset als Subset abgespeichert. Dieses Subset ist zudem in dem GitHub Repository enthalten. Die Ergebnisse können dadurch bei der Evaluierug auch ohne herunterladen, entzippen und Datenaufbereitung des Wikipedia Dumps reproduziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove existing subset\n",
      "generate subset folders\n",
      "read meta data\n",
      "undersample original dataset\n",
      "split dataset\n",
      "save articles\n",
      "write new meta csv\n",
      "----\n",
      "Gesamtanzahl der Artikel: 2496675\n",
      "Anzahl der Samples im Subset (Testdaten): 807\n",
      "Klassenverteilung: Counter({True: 411, False: 396})\n"
     ]
    }
   ],
   "source": [
    "# generate subset equivalent to the test dataset \n",
    "print('remove existing subset')\n",
    "if os.path.exists(SUBSET_FOLDER):\n",
    "    shutil.rmtree(SUBSET_FOLDER)\n",
    "print('generate subset folders')\n",
    "os.makedirs(SUBSET_FOLDER)\n",
    "os.makedirs(SUBSET_FOLDER+'/exzellent')\n",
    "os.makedirs(SUBSET_FOLDER+'/not_exzellent')\n",
    "\n",
    "# read filenams from meta csv\n",
    "print('read meta data')\n",
    "original_meta_data = pd.read_csv(CSV_FILE, header=0, index_col=0)\n",
    "X = original_meta_data.drop(['is_excellent'], axis=1)\n",
    "Y = original_meta_data['is_excellent']\n",
    "\n",
    "# undersample data for same number of text per class \n",
    "print('undersample original dataset')\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X, Y = rus.fit_resample(X, Y)\n",
    "\n",
    "# gernerate train, val, test set\n",
    "print('split dataset')\n",
    "_, X_test_val, _, Y_test_val = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_test, _, Y_test, _ = train_test_split(X_test_val, Y_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "# concat x and y to df\n",
    "dataset = pd.concat([X_test,Y_test], axis=1)\n",
    "\n",
    "# save articles in subset folders\n",
    "print('save articles')\n",
    "for article_id, article in dataset.iterrows():\n",
    "    # get folder name based on label\n",
    "    ordner_original = EXZELLENT_FOLDER if article['is_excellent'] else NOT_EXZELLENT_FOLDER\n",
    "    ordner_subset = SUBSET_FOLDER + str('/exzellent' if article['is_excellent'] else '/not_exzellent')\n",
    "    # merge filename\n",
    "    filename = str(article_id) + '.txt'\n",
    "    # join filepath\n",
    "    filepath_original = os.path.join(ordner_original, filename)\n",
    "    filepath_subset = os.path.join(ordner_subset, filename)\n",
    "        \n",
    "    # copy original file to subset folder\n",
    "    shutil.copyfile(filepath_original, filepath_subset)\n",
    "\n",
    "# write meta csv for the subset\n",
    "print('write new meta csv')\n",
    "dataset.to_csv(os.path.join(SUBSET_FOLDER,'articles_meta.csv'), mode='w+')\n",
    "\n",
    "# print information about the subset\n",
    "print('----')\n",
    "print('Gesamtanzahl der Artikel: %s' % len(original_meta_data))\n",
    "print('Anzahl der Samples im Subset (Testdaten): %i' % len(Y_test))\n",
    "print('Klassenverteilung: %s' % Counter(Y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
